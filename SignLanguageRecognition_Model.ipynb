{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahananda123/SignLanguageRecognitionUsingDeepLEarning/blob/main/SignLanguageRecognition_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "CBrbjY6AOx-L",
        "outputId": "aeb5f78f-7080-40c1-f1d5-8e0220e2c88e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.15.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrtN6tM88TCZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.applications import VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn14vo-N8YzG",
        "outputId": "fd37c15e-49bd-408c-87e8-1565d856572b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 35142 files belonging to 36 classes.\n",
            "Using 28114 files for training.\n",
            "Found 35142 files belonging to 36 classes.\n",
            "Using 7028 files for validation.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Specify the path to your custom dataset\n",
        "data_path = '/content/drive/MyDrive/CNN Architecture/original_images'\n",
        "\n",
        "# Load the data\n",
        "train_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_path,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    image_size=(225,225),\n",
        "    batch_size=50,\n",
        "    validation_split=0.2,  # Use validation_split for splitting into training and validation sets\n",
        "    subset='training',  # Subset for training data\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "test_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_path,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    image_size=(225,225),\n",
        "    batch_size=20,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',  # Subset for validation data\n",
        "    seed=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvjCiF6apiRS"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data\n",
        "train_data = train_data.map(lambda x, y: (x / 255.0, y))\n",
        "test_data = test_data.map(lambda x, y: (x / 255.0, y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zxAs6fBpoZe",
        "outputId": "79ecdb73-5beb-4933-ae34-c2ed5f704415"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the pre-trained VGG16 model without the top (fully connected) layers\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(225, 225, 3))\n",
        "\n",
        "# Freeze the pre-trained layers so they are not updated during training\n",
        "base_model.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YI7OpkqepqcK"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16, kernel_size=(2,2), activation='relu', input_shape=(225, 225, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',input_shape=(225, 225, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(36, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij2SiqgLpt7V"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSR2OYNlpwU3",
        "outputId": "15474efa-d66b-4d26-d847-67006e3c011f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            " 10/563 [..............................] - ETA: 38:25 - loss: 3.5940 - accuracy: 0.0300"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "model_hist=model.fit(train_data,validation_data=(test_data),epochs=70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHfOe1ebp03f"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svChbmzQuNp5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(model_hist.history['loss'])\n",
        "plt.plot(model_hist.history['val_loss'])\n",
        "plt.title('model_loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','validation'],loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ2p0VHx-dLb"
      },
      "outputs": [],
      "source": [
        "plt.plot(model_hist.history['accuracy'])\n",
        "plt.plot(model_hist.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMVt_l1ZFl-c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have a test dataset named test_data\n",
        "test_images = []  # List to store test images\n",
        "\n",
        "# Assuming test_data is a tf.data.Dataset\n",
        "for images, labels in test_data:\n",
        "    test_images.append(images)\n",
        "\n",
        "# Concatenate test images into a single tensor\n",
        "test_images = tf.concat(test_images, axis=0)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(test_images)\n",
        "\n",
        "# Convert predictions to class labels\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Now you have the predicted labels for your test data\n",
        "print(\"Predicted Labels:\", predicted_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0My7O0JDF6AM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Convert test_data to a NumPy array or TensorFlow tensor\n",
        "test_images = []\n",
        "test_labels = []\n",
        "for images, labels in test_data:\n",
        "    test_images.append(images)\n",
        "    test_labels.append(labels)\n",
        "\n",
        "test_images = tf.concat(test_images, axis=0)\n",
        "true_labels = tf.argmax(tf.concat(test_labels, axis=0), axis=1)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(test_images)\n",
        "\n",
        "# Convert predictions to class labels\n",
        "predicted_labels = tf.argmax(predictions, axis=1)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "print(\"F1 Score:\", f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4Mz1iS_GMUP"
      },
      "outputs": [],
      "source": [
        "print(len(predicted_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXCsfhq-BDOO"
      },
      "outputs": [],
      "source": [
        "model_hist.save('research')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PNc-D23_INU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jANaABhQ3mT"
      },
      "outputs": [],
      "source": [
        "from colabcode import Colabcode\n",
        "Colabcode(port=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaPlI4m9xH3y",
        "outputId": "b24c9457-d735-43fa-9706-88e5d5c0d6d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7iqh-LmxLqH",
        "outputId": "420cc60d-db2d-4181-f18b-17e7c660fc4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "5.354825623000011\n",
            "GPU (s):\n",
            "0.14207483399999887\n",
            "GPU speedup over CPU: 37x\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "\n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIEbb6QPxViU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "11MBosePAmXea92H2XzOM8x7Af0UK9Qq_",
      "authorship_tag": "ABX9TyOvKnUyXrYR2sybPIvrjqQz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}